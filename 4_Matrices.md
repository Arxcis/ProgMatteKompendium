[TOC]

# 4. Matrices

## 4.1 Vectors

**Linear combination** - A linear combination is a vector consisting of other vectors scaled linearly. Thinking about vectors as linear combinations, we find that there are only a few vectors that exist that are not linear combinations (linearly independent vectors). In an $n$-dimensional space there are only $n$ vectors that are not linear combinations, being the $n$ basis vectors of said space.
$$
\vec{v\Sigma} = a0\vec{v_0}+\dots+a_n\vec{v_n}
$$
**Dot product** - Dot product 

*Geometric definition*: $$ \vec{u} \cdot \vec{v} = \|u\|\space\|v\|\cos\theta $$

*Algebraic definition*: $$\vec{v} \cdot \vec{u} = \sum_{i=1}^nv_iu_i$$

**Length**

**Unit vector**

**Angle**

## 4.2 Vector spaces

**Sub space**

**Linear independence**

**Basis**

**Orthonormal basis**

**Dimension**

**Column space**

**Nullspace**

**Rank**

## 4.3 Othogonality

**Projection**

**Least-squares approximation**

**Pseudoinverse**

## 4.4 Eigen values and Eigen Vectors

**Eigenvalue - Eigenvector**

**Diagonalization**

**Symmetric matrix**

**Similar matrix**


